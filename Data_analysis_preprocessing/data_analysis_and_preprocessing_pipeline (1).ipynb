{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "_7tDVUyO6rsa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipaddress\n",
        "import logging\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "z-gr22fC64Oi"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ Load datasets\n",
        "def load_data():\n",
        "    try:\n",
        "        fraud_data = pd.read_csv(\"/content/drive/MyDrive/weak_8_data/Data-20250205T200552Z-001/Data/Fraud_Data.csv\")\n",
        "        ip_country_data = pd.read_csv(\"/content/drive/MyDrive/weak_8_data/Data-20250205T200552Z-001/Data/IpAddress_to_Country.csv\")\n",
        "        #credit_data=pd.read_csv(\"/content/drive/MyDrive/weak_8_data/Data-20250205T200552Z-001/Data/IpAddress_to_Country.csv\")\n",
        "        logger.info(f\"Loaded Fraud_Data.csv with shape {fraud_data.shape}\")\n",
        "        logger.info(f\"Loaded IpAddress_to_Country.csv with shape {ip_country_data.shape}\")\n",
        "        #logger.info(f\"Loaded Credit data with shape {credit_data.shape}\")\n",
        "        return fraud_data, ip_country_data # credit_data\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading datasets: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "hnS2AD6G6520"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ Handle missing values\n",
        "def handle_missing_values(df):\n",
        "    missing_before = df.isnull().sum()\n",
        "    df.fillna({\n",
        "        \"browser\": \"Unknown\",\n",
        "        \"source\": \"Unknown\",\n",
        "        \"purchase_value\": df[\"purchase_value\"].median(),\n",
        "    }, inplace=True)\n",
        "    logger.info(f\"Handled missing values. Before: {missing_before.sum()}, After: {df.isnull().sum().sum()}\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "f52r0AAr7PJE"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ Data cleaning: remove duplicates, correct types\n",
        "def clean_data(df):\n",
        "    before = df.shape[0]\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    df[\"purchase_value\"] = pd.to_numeric(df[\"purchase_value\"], errors=\"coerce\")\n",
        "    df[\"signup_time\"] = pd.to_datetime(df[\"signup_time\"], errors=\"coerce\")\n",
        "    df[\"purchase_time\"] = pd.to_datetime(df[\"purchase_time\"], errors=\"coerce\")\n",
        "    logger.info(f\"Removed {before - df.shape[0]} duplicate rows.\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "k6Cx--Xx8f5M"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ Merge IP address data with a proper range-based lookup\n",
        "def merge_geolocation(fraud_df, ip_df):\n",
        "    try:\n",
        "        # Convert columns to numeric safely\n",
        "        fraud_df[\"ip_address\"] = pd.to_numeric(fraud_df[\"ip_address\"], errors=\"coerce\")\n",
        "        ip_df[\"lower_bound_ip_address\"] = pd.to_numeric(ip_df[\"lower_bound_ip_address\"], errors=\"coerce\")\n",
        "        ip_df[\"upper_bound_ip_address\"] = pd.to_numeric(ip_df[\"upper_bound_ip_address\"], errors=\"coerce\")\n",
        "\n",
        "        # Drop NaNs to avoid errors\n",
        "        fraud_df.dropna(subset=[\"ip_address\"], inplace=True)\n",
        "        ip_df.dropna(subset=[\"lower_bound_ip_address\", \"upper_bound_ip_address\"], inplace=True)\n",
        "\n",
        "        # Use integer type if possible\n",
        "        fraud_df[\"ip_address\"] = fraud_df[\"ip_address\"].astype(\"int\")\n",
        "        ip_df[\"lower_bound_ip_address\"] = ip_df[\"lower_bound_ip_address\"].astype(\"int\")\n",
        "        ip_df[\"upper_bound_ip_address\"] = ip_df[\"upper_bound_ip_address\"].astype(\"int\")\n",
        "\n",
        "        # Perform range-based lookup\n",
        "        def find_country(ip):\n",
        "            match = ip_df[(ip_df[\"lower_bound_ip_address\"] <= ip) & (ip_df[\"upper_bound_ip_address\"] >= ip)]\n",
        "            return match[\"country\"].values[0] if not match.empty else \"Unknown\"\n",
        "\n",
        "        fraud_df[\"country\"] = fraud_df[\"ip_address\"].apply(find_country)\n",
        "\n",
        "        logger.info(f\"Merged geolocation data successfully. New shape: {fraud_df.shape}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in merging geolocation data: {e}\")\n",
        "\n",
        "    return fraud_df\n",
        "\n"
      ],
      "metadata": {
        "id": "BAltkIx68jtO"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering(df):\n",
        "    try:\n",
        "        # Ensure purchase_time is datetime\n",
        "        df[\"purchase_time\"] = pd.to_datetime(df[\"purchase_time\"], errors=\"coerce\")\n",
        "\n",
        "        # Remove rows with missing timestamps\n",
        "        df.dropna(subset=[\"purchase_time\"], inplace=True)\n",
        "\n",
        "        # Calculate transaction velocity (time difference between transactions)\n",
        "        df[\"transaction_velocity\"] = df.groupby(\"user_id\")[\"purchase_time\"].diff().dt.total_seconds()\n",
        "\n",
        "        # Fix: Assign back to df explicitly to avoid FutureWarning\n",
        "        df[\"transaction_velocity\"] = df[\"transaction_velocity\"].fillna(86400)\n",
        "\n",
        "        # Calculate transaction frequency (count of transactions per user)\n",
        "        df[\"transaction_frequency\"] = df.groupby(\"user_id\")[\"user_id\"].transform(\"count\")\n",
        "\n",
        "        # Extract time-based features\n",
        "        df[\"hour_of_day\"] = df[\"purchase_time\"].dt.hour\n",
        "        df[\"day_of_week\"] = df[\"purchase_time\"].dt.weekday\n",
        "\n",
        "        logger.info(\"Created new features: transaction velocity, transaction frequency, hour_of_day, day_of_week\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in feature engineering: {e}\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "3rRcFUjv8nid"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ Normalize & Encode\n",
        "def normalize_encode(df):\n",
        "    try:\n",
        "        if df.shape[0] == 0:\n",
        "            logger.warning(\"No data available for normalization.\")\n",
        "            return df\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        df[\"purchase_value\"] = scaler.fit_transform(df[[\"purchase_value\"]])\n",
        "\n",
        "        encoder = LabelEncoder()\n",
        "        df[\"browser\"] = encoder.fit_transform(df[\"browser\"])\n",
        "        df[\"source\"] = encoder.fit_transform(df[\"source\"])\n",
        "        df[\"sex\"] = encoder.fit_transform(df[\"sex\"])\n",
        "\n",
        "        logger.info(\"Normalized purchase_value and encoded categorical features.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in normalization and encoding: {e}\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "ZBipf7sA8sxH"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”¹ Full Pipeline Execution\n",
        "def fraud_detection_pipeline():\n",
        "    fraud_data, ip_country_data = load_data()\n",
        "    fraud_data = handle_missing_values(fraud_data)\n",
        "    fraud_data = clean_data(fraud_data)\n",
        "    fraud_data = merge_geolocation(fraud_data, ip_country_data)\n",
        "    fraud_data = feature_engineering(fraud_data)\n",
        "    fraud_data = normalize_encode(fraud_data)\n",
        "\n",
        "    logger.info(f\"Final dataset shape: {fraud_data.shape}\")\n",
        "    return fraud_data\n"
      ],
      "metadata": {
        "id": "mrJvEZAw8wIq"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸš€ Run the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    final_df = fraud_detection_pipeline()\n",
        "    final_df.to_csv(\"Preprocessed_Fraud_Data.csv\", index=False)\n",
        "    logger.info(\"Saved preprocessed data to Preprocessed_Fraud_Data.csv\")"
      ],
      "metadata": {
        "id": "l8C4CONf80CH"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZN0RAU8wuN8",
        "outputId": "fc98d167-bd2a-4d61-a282-21f9b48a49f4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   user_id         signup_time       purchase_time  purchase_value  \\\n",
            "0    22058 2015-02-24 22:55:49 2015-04-18 02:47:11       -0.160204   \n",
            "1   333320 2015-06-07 20:39:50 2015-06-08 01:38:54       -1.142592   \n",
            "2     1359 2015-01-01 18:52:44 2015-01-01 18:52:45       -1.197169   \n",
            "3   150084 2015-04-28 21:13:25 2015-05-04 13:54:50        0.385567   \n",
            "4   221365 2015-07-21 07:09:52 2015-09-09 18:40:53        0.112681   \n",
            "\n",
            "       device_id  source  browser  sex  age  ip_address  class        country  \\\n",
            "0  QVPSPJUOCKZAR       2        0    1   39   732758368      0          Japan   \n",
            "1  EOGFQPIZPYXFZ       0        0    0   53   350311387      0  United States   \n",
            "2  YSSKYOSJHPPLJ       2        3    1   53  2621473820      1  United States   \n",
            "3  ATGTXKYKUDUQN       2        4    1   41  3840542443      0        Unknown   \n",
            "4  NAUITBZFJKHWW       0        4    1   45   415583117      0  United States   \n",
            "\n",
            "   transaction_velocity  transaction_frequency  hour_of_day  day_of_week  \n",
            "0               86400.0                      1            2            5  \n",
            "1               86400.0                      1            1            0  \n",
            "2               86400.0                      1           18            3  \n",
            "3               86400.0                      1           13            0  \n",
            "4               86400.0                      1           18            2  \n"
          ]
        }
      ]
    }
  ]
}